// SPDX-License-Identifier: PMPL-1.0-or-later
= Axiom.jl
:toc: macro
:toclevels: 2
:icons: font
:source-highlighter: rouge

image:https://img.shields.io/badge/License-PMPL--1.0-blue.svg[License: PMPL-1.0,link="https://github.com/hyperpolymath/palimpsest-license"]
image:https://img.shields.io/badge/Julia-1.10+-9558B2?logo=julia[Julia,link="https://julialang.org/"]
image:https://img.shields.io/badge/Zig-Backend-F7A41D?logo=zig[Zig,link="https://ziglang.org/"]

*Provably Correct Machine Learning*

_Machine learning where bugs are caught at compile time, not in production._

toc::[]

== What is Axiom.jl?

Axiom.jl is a *next-generation ML framework* that combines:

* *Compile-time verification* - Shape errors caught before runtime
* *Formal guarantees* - Verification checks and certificate workflows
* *Optional acceleration* - Zig/GPU backend paths with explicit fallback behavior
* *Julia elegance* - Express models as mathematical specifications

[source,julia]
----
using Axiom

model = Sequential(
    Dense(784, 128, relu),
    Dense(128, 10),
    Softmax()
)

x = Tensor(randn(Float32, 16, 784))
y = model(x)
result = verify(model, properties=[ValidProbabilities(), FiniteOutput()], data=[(x, nothing)])
@assert result.passed
----

== Features

=== Compile-Time Shape Verification

[source,julia]
----
# PyTorch: Runtime error after hours of training
# Axiom.jl: Compile error in milliseconds

@axiom BrokenModel begin
    input :: Tensor{Float32, (224, 224, 3)}
    features = input |> Conv(64, (3,3))
    output = features |> Dense(10)  # COMPILE ERROR!
    # "Shape mismatch: Conv output is (222,222,64), Dense expects vector"
end
----

=== Formal Verification

[source,julia]
----
@axiom SafeClassifier begin
    # ...
    @ensure valid_probabilities(output)    # Runtime assertion
    @prove ∀x. sum(softmax(x)) == 1.0      # Experimental proof workflow
end

# Generate verification certificates
cert = verify(model) |> generate_certificate
save_certificate(cert, "fda_submission.cert")
----

=== Model Interoperability

[source,julia]
----
# Import from a PyTorch checkpoint (.pt/.pth/.ckpt) via built-in Python bridge
# (requires python3 + torch in the selected runtime)
model = from_pytorch("model.pt")

# Or import canonical descriptor JSON
model2 = from_pytorch("model.pytorch.json")

# Export supported models to ONNX
to_onnx(model, "model.onnx", input_shape=(1, 3, 224, 224))
----

Current scope:

* `from_pytorch(...)`: canonical descriptor import + direct `.pt/.pth/.ckpt` bridge.
* `to_onnx(...)`: export for `Sequential`/`Pipeline` models built from Dense/Conv/Norm/Pool + common activations.

=== High Performance

[source,julia]
----
# Development: Julia backend
model = Sequential(Dense(784, 128, relu), Dense(128, 10))

# Production path: optional Zig backend
prod_model = compile(model, backend=ZigBackend("/path/to/libaxiom_zig.so"), optimize=:aggressive)
----

=== Coprocessor Targets

[source,julia]
----
# Non-GPU accelerator targets with self-healing fallback
cop = detect_coprocessor()  # TPU/NPU/VPU/QPU/PPU/MATH/CRYPTO/FPGA/DSP or nothing
if cop !== nothing
    model_accel = compile(model, backend=cop, verify=false)
end
----

=== Model Packaging + Registry Manifests

[source,julia]
----
metadata = create_metadata(
    model;
    name="my-model",
    architecture="Sequential",
    version="1.0.0",
)
verify_and_claim!(metadata, "FiniteOutput", "verified=true; source=ci")

bundle = export_model_package(model, metadata, "build/model_package")
entry = build_registry_entry(bundle["manifest"]; channel="stable")
export_registry_entry(entry, "build/model_package/registry-entry.json")
----

=== Verification Telemetry

[source,julia]
----
reset_verification_telemetry!()
result = verify(model, properties=[FiniteOutput()], data=[(x, nothing)])
run_payload = verification_result_telemetry(result; source="inference-gate")
summary = verification_telemetry_report()
----

=== Serving APIs

[source,julia]
----
# REST
rest_server = serve_rest(model; host="0.0.0.0", port=8080, background=true)

# GraphQL
graphql_server = serve_graphql(model; host="0.0.0.0", port=8081, background=true)

# gRPC bridge server + contract generation
# - binary unary protobuf (`application/grpc`)
# - JSON bridge mode (`application/grpc+json`)
grpc_server = serve_grpc(model; host="0.0.0.0", port=50051, background=true)
generate_grpc_proto("axiom_inference.proto")
----

=== Interop APIs

[source,julia]
----
# PyTorch import (checkpoint bridge or canonical descriptor JSON)
model = from_pytorch("model.pt")
model = from_pytorch("model.pytorch.json")

# ONNX export (Dense/Conv/Norm/Pool + common activations)
to_onnx(model, "model.onnx", input_shape=(1, 3, 224, 224))
----

== Quick Start

=== Installation

[source,julia]
----
using Pkg
Pkg.add("Axiom")
----

=== Hello World

[source,julia]
----
using Axiom

# Define a simple classifier
model = Sequential(
    Dense(784, 256, relu),
    Dense(256, 10),
    Softmax()
)

# Generate sample data
x = randn(Float32, 32, 784)

# Inference
predictions = model(x)

# Verify properties
@ensure all(sum(predictions, dims=2) .≈ 1.0)
----

=== With @axiom DSL

[source,julia]
----
using Axiom

@axiom MNISTClassifier begin
    input :: Tensor{Float32, (:batch, 28, 28, 1)}
    output :: Probabilities(10)

    features = input |> Conv(32, (3,3)) |> ReLU |> MaxPool((2,2))
    features = features |> Conv(64, (3,3)) |> ReLU |> MaxPool((2,2))
    flat = features |> GlobalAvgPool() |> Flatten
    output = flat |> Dense(64, 10) |> Softmax

    @ensure valid_probabilities(output)
end

model = MNISTClassifier()
----

== Why Axiom.jl?

=== The Problem

ML models are deployed in critical applications:

* Medical diagnosis
* Autonomous vehicles
* Financial systems
* Criminal justice

Yet our tools allow bugs to slip through to production.

=== The Solution

Axiom.jl catches bugs *before* they cause harm:

[cols="1,1,1",options="header"]
|===
| Issue | PyTorch | Axiom.jl

| Shape mismatch
| Runtime crash
| Compile error

| NaN in output
| Silent failure
| Detected/proven

| Invalid probabilities
| Undetected
| Checkable with verification properties

| Adversarial fragility
| Unknown
| Roadmap / partial
|===

== Documentation

* link:docs/wiki/Home.md[Home] - Start here
* link:docs/wiki/User-Guide.md[User Guide] - Install, infer, verify
* link:docs/wiki/Developer-Guide.md[Developer Guide] - Build/test/release workflow
* link:RELEASE-CHECKLIST.adoc[Release Checklist] - Pre-release and release-day gates
* link:docs/wiki/Vision.md[Vision] - Why we built this
* link:docs/wiki/Axiom-DSL.md[@axiom DSL] - Model definition guide
* link:docs/wiki/Verification.md[Verification] - @ensure and @prove
* link:docs/wiki/Migration-Guide.md[Migration Guide] - From PyTorch
* link:docs/wiki/FAQ.md[FAQ] - Common questions
* link:ROADMAP.md[Roadmap] - Tracked commitments and delivery criteria

== Project Structure

[source]
----
Axiom.jl/
├── src/                 # Julia source
│   ├── Axiom.jl        # Main module
│   ├── types/          # Tensor type system
│   ├── layers/         # Neural network layers
│   ├── dsl/            # @axiom macro system
│   ├── verification/   # @ensure, @prove
│   ├── training/       # Optimizers, loss functions
│   └── backends/       # Backend abstraction (15 backends)
├── zig/                # Zig native backend
│   └── src/           # matmul, conv, norm, attention, etc.
├── ext/                # GPU package extensions (CUDA, ROCm, Metal)
├── test/               # Test suite
├── examples/           # Example models
└── docs/               # Documentation & wiki
----

== Roadmap

* [x] *v0.1* - Core framework, DSL, verification basics
* [ ] *v0.2* - Full Zig backend, GPU support
* [ ] *v0.3* - Hugging Face integration, model zoo
* [ ] *v0.4* - Advanced proofs, SMT integration
* [ ] *v1.0* - Production ready, industry certifications

== Contributing

We welcome contributions! See link:CONTRIBUTING.md[CONTRIBUTING.md].

* Bug reports and feature requests
* Documentation improvements
* New layers and operations
* Performance optimizations
* Verification methods

== Julia-First Verification

Axiom's proof system is *Julia-native by default*. SMT solving runs through
`packages/SMTLib.jl` with no native backend dependency. The Zig SMT runner is an optional
backend you can enable for hardened subprocess control.

Julia-native example:

[source,julia]
----
@prove ∃x. x > 0
----

Optional Zig runner:

[source,bash]
----
export AXIOM_SMT_RUNNER=zig
export AXIOM_ZIG_LIB=/path/to/libaxiom_zig.so
export AXIOM_SMT_SOLVER=z3
----

[source,julia]
----
@prove ∃x. x > 0
----

== License

Palimpsest-MPL-1.0 License - see link:LICENSE[LICENSE] for details.

== Acknowledgments

Axiom.jl builds on the shoulders of giants:

* https://julialang.org/[Julia] - The language
* https://fluxml.ai/[Flux.jl] - Inspiration for Julia ML
* https://ziglang.org/[Zig] - Native performance backend
* https://pytorch.org/[PyTorch] - Ecosystem compatibility

'''

_The future of ML is verified._

link:docs/wiki/Home.md[Get Started] | link:RELEASE-CHECKLIST.adoc[Release Checklist] | link:ROADMAP.md[Roadmap] | https://github.com/hyperpolymath/Axiom.jl[Star on GitHub]
